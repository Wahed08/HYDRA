{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Source Code for HYDRA with across all models.**"
      ],
      "metadata": {
        "id": "YwtgBcTxPF1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HYDRA** Model"
      ],
      "metadata": {
        "id": "leSQyv6ZPdZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from itertools import product\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "# Set reproducibility seed\n",
        "np.random.seed(42)  # NumPy seed\n",
        "torch.manual_seed(42)  # PyTorch seed (for CUDA, add torch.cuda.manual_seed_all(42) if using GPU)\n",
        "\n",
        "# VAE Model (Base Case)\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_layer = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(128, 64)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(64, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder_layer(x)\n",
        "        mu = self.fc1(x)\n",
        "        log_var = self.fc2(x)\n",
        "        return mu, log_var\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std) * 0.001\n",
        "        z = mu + eps * std\n",
        "        return z\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        x_reconstructed = self.decode(z)\n",
        "        return x_reconstructed, mu, log_var\n",
        "\n",
        "def loss_function(recon_x, x, mu, log_var):\n",
        "    MSE = F.mse_loss(recon_x, x, reduction='mean')\n",
        "    return MSE\n",
        "\n",
        "# GraphCodeBERT Feature Extraction\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
        "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\")\n",
        "\n",
        "def get_code_features(code):\n",
        "    inputs = tokenizer(code, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "# Heuristic Detection Functions\n",
        "def detect_missing_malloc_check(code: str) -> int:\n",
        "    alloc_pattern = r'\\b(\\w+)\\s*\\*\\s*(\\w+)\\s*=\\s*\\([^\\)]*\\)\\s*(malloc|calloc|realloc)\\s*\\([^;]+?\\);|\\b(\\w+)\\s*\\*\\s*(\\w+)\\s*=\\s*(malloc|calloc|realloc)\\s*\\([^;]+?\\);'\n",
        "    matches = re.findall(alloc_pattern, code)\n",
        "    ptrs = [match[1] if match[1] else match[4] for match in matches if match[1] or match[4]]\n",
        "    for ptr in ptrs:\n",
        "        is_checked = False\n",
        "        is_used = False\n",
        "        null_check_patterns = [rf'if\\s*\\(\\s*{ptr}\\s*\\)', rf'if\\s*\\(\\s*{ptr}\\s*!=\\s*NULL\\s*\\)', rf'if\\s*\\(\\s*!{ptr}\\s*\\)', rf'if\\s*\\(\\s*NULL\\s*!=\\s*{ptr}\\s*\\)']\n",
        "        usage_patterns = [rf'{ptr}\\s*\\[', rf'\\*\\s*{ptr}', rf'{ptr}\\s*->', rf'{ptr}\\s*\\(', rf'{ptr}\\s*=']\n",
        "        for pat in null_check_patterns:\n",
        "            if re.search(pat, code):\n",
        "                is_checked = True\n",
        "                break\n",
        "        for pat in usage_patterns:\n",
        "            if re.search(pat, code):\n",
        "                is_used = True\n",
        "                break\n",
        "        if is_used and not is_checked:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def missing_null_check_func(code):\n",
        "    functions = re.findall(r'\\b\\w+\\s+\\w+\\s*\\([^)]*\\)\\s*\\{[^{}]*\\}', code, re.DOTALL)\n",
        "    for func in functions:\n",
        "        ptr_decl = re.findall(r'\\b\\w+\\s*\\*\\s*(\\w+)\\s*=\\s*[^;]+;', func)\n",
        "        ptr_func_decl = re.findall(r'\\b\\w+\\s*\\*\\s*(\\w+)\\s*;', func)\n",
        "        ptr_vars = list(set(ptr_decl + ptr_func_decl))\n",
        "        for ptr in ptr_vars:\n",
        "            unsafe_use = re.search(rf'[^a-zA-Z0-9_]({ptr})(->|\\[\\d*]|[^\\w]?\\*)|\\b{ptr}\\s*\\(', func)\n",
        "            if not unsafe_use:\n",
        "                continue\n",
        "            null_check = re.search(rf'if\\s*\\(\\s*(!\\s*{ptr}|{ptr}\\s*==\\s*NULL|{ptr}\\s*!=\\s*NULL)\\s*\\)', func)\n",
        "            if not null_check:\n",
        "                return 1\n",
        "    return 0\n",
        "\n",
        "def detect_race_condition(code: str) -> bool:\n",
        "    field_assignment_pattern = r'\\b\\w+\\s*(->|\\.)\\s*\\w+\\s*=.*?;'\n",
        "    control_block_pattern = r'\\b(if|while|for|switch)\\s*\\([^\\)]+\\)\\s*{[^}]*' + field_assignment_pattern + r'[^}]*}'\n",
        "    matches_control_blocks = re.findall(control_block_pattern, code, re.DOTALL)\n",
        "    locking_primitive_pattern = r'\\b(mutex_lock|pthread_mutex_lock|spin_lock)\\b'\n",
        "    unlocking_primitive_pattern = r'\\b(mutex_unlock|pthread_mutex_unlock|spin_unlock)\\b'\n",
        "    has_locking = re.search(locking_primitive_pattern, code)\n",
        "    has_unlocking = re.search(unlocking_primitive_pattern, code)\n",
        "    if matches_control_blocks and not (has_locking and has_unlocking):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def logging_but_no_blocking(code):\n",
        "    log_lines = re.findall(r'(syslog\\([^)]*\\)|printk\\([^)]*\\))', code)\n",
        "    for line in log_lines:\n",
        "        log_index = code.find(line)\n",
        "        snippet = code[log_index:log_index+150]\n",
        "        if not re.search(r'(return|exit|break|continue)', snippet):\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def split_into_functions(code):\n",
        "    functions = re.findall(r'([a-zA-Z_][a-zA-Z0-9_\\*\\s]*\\s+[a-zA-Z_][a-zA-Z0-9_]*\\s*\\(.*?\\)\\s*\\{.*?\\})', code, re.DOTALL)\n",
        "    return functions\n",
        "\n",
        "def missing_bounds_check(code):\n",
        "    functions = split_into_functions(code)\n",
        "    risky_keywords = ['recv', 'read', 'strcpy', 'memcpy', 'gets', 'strcat', 'write']\n",
        "    for func in functions:\n",
        "        found_risky = any(kw in func for kw in risky_keywords)\n",
        "        has_check = any('if' in line and ('<' in line or '>' in line or '<=' in line or '>=' in line) for line in func.splitlines())\n",
        "        if found_risky and not has_check:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def analyze_risks(code):\n",
        "    risk_flags = {\n",
        "        \"Missing Null Check\": missing_null_check_func(code),\n",
        "        \"Race Condition\": detect_race_condition(code),\n",
        "        \"Missing Bounds Check\": missing_bounds_check(code),\n",
        "        \"Unsafe Memory Allocation\": detect_missing_malloc_check(code),\n",
        "        \"Logging Without Halting\": logging_but_no_blocking(code),\n",
        "        \"issue_detected\": 0\n",
        "    }\n",
        "    if any(v != 0 for v in risk_flags.values() if v is not False):\n",
        "        risk_flags[\"issue_detected\"] = 1\n",
        "    return risk_flags\n",
        "\n",
        "# Load and preprocess data\n",
        "train_path = '/home/phoenix/Desktop/TA/Re_ New Code/Linux-Data-Updated.csv'\n",
        "test_paths = {\n",
        "    'Android': '/home/phoenix/Desktop/TA/Re_ New Code/CSV Data of three testing projects/Android-Data.csv',\n",
        "    'Chrome': '/home/phoenix/Desktop/TA/Re_ New Code/CSV Data of three testing projects/Chrome-Data.csv',\n",
        "    'ImageMagick': '/home/phoenix/Desktop/TA/Re_ New Code/CSV Data of three testing projects/ImageMagick-Data.csv'\n",
        "}\n",
        "\n",
        "def load_or_generate_embeddings(df, path, filename):\n",
        "    embed_file = f\"{filename}_embeddings.npy\"\n",
        "    heur_file = f\"{filename}_heuristics.npy\"\n",
        "    commit_file = f\"{filename}_commit_ids.npy\"\n",
        "\n",
        "    if os.path.exists(embed_file) and os.path.exists(heur_file) and os.path.exists(commit_file):\n",
        "        embeddings = np.load(embed_file)\n",
        "        heuristics = np.load(heur_file)\n",
        "        commit_ids = np.load(commit_file)\n",
        "    else:\n",
        "        embeddings = []\n",
        "        heuristics = []\n",
        "        commit_ids = []\n",
        "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing GraphCodeBERT embeddings and heuristics ({filename})\"):\n",
        "            code = row['PCode']\n",
        "            embedding = get_code_features(code)\n",
        "            risk_flags = analyze_risks(code)\n",
        "            embeddings.append(embedding)\n",
        "            heuristics.append([risk_flags[h] for h in [\n",
        "                \"Missing Null Check\", \"Race Condition\", \"Missing Bounds Check\",\n",
        "                \"Unsafe Memory Allocation\", \"Logging Without Halting\", \"issue_detected\"\n",
        "            ]])\n",
        "            commit_ids.append(row['Commit_Id'])\n",
        "        embeddings = np.array(embeddings)\n",
        "        heuristics = np.array(heuristics)\n",
        "        np.save(embed_file, embeddings)\n",
        "        np.save(heur_file, heuristics)\n",
        "        np.save(commit_file, commit_ids)\n",
        "    return embeddings, heuristics, commit_ids\n",
        "\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_train['PCode'] = df_train['PCode'].astype(str)\n",
        "embeddings_train, heuristics_train, commit_ids_train = load_or_generate_embeddings(df_train, train_path, \"train\")\n",
        "\n",
        "# Combine embeddings and heuristics for training and split for validation\n",
        "features_train = np.concatenate([embeddings_train, heuristics_train], axis=1)\n",
        "train_data, val_data = train_test_split(features_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Hyperparameter tuning for latent_dim and n_clusters\n",
        "input_dim = features_train.shape[1]  # 774 (768 + 6)\n",
        "output_dim = input_dim\n",
        "param_grid = {\n",
        "    'latent_dim': [5, 10, 20],\n",
        "    'n_clusters': [2, 3, 5]\n",
        "}\n",
        "results = []\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "for params in tqdm(product(*param_grid.values()), desc=\"Tuning VAE and K-means\"):\n",
        "    latent_dim, n_clusters = params\n",
        "    vae = VAE(input_dim, output_dim).to(device)\n",
        "    optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    criterion = loss_function\n",
        "    vae.train()\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 20\n",
        "    trigger_times = 0\n",
        "    for epoch in range(200):\n",
        "        vae.train()\n",
        "        total_loss = 0\n",
        "        total_recon_loss = 0\n",
        "        for i in range(0, len(train_data), 128):\n",
        "            batch = torch.FloatTensor(train_data[i:i+128]).to(device)\n",
        "            optimizer.zero_grad()\n",
        "            recon_batch, mu, log_var = vae(batch)\n",
        "            recon_loss = criterion(recon_batch, batch, mu, log_var)\n",
        "            recon_loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += recon_loss.item()\n",
        "            total_recon_loss += recon_loss.item()\n",
        "        # Validation phase\n",
        "        vae.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(val_data), 128):\n",
        "                val_batch = torch.FloatTensor(val_data[i:i+128]).to(device)\n",
        "                recon_val_batch, _, _ = vae(val_batch)\n",
        "                val_loss += criterion(recon_val_batch, val_batch, mu, log_var).item()\n",
        "        avg_val_loss = val_loss / (len(val_data) / 128)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            avg_recon_loss = total_recon_loss / (len(train_data) / 128)\n",
        "            print(f\"Epoch [{epoch+1}/200], Latent Dim={latent_dim}, Train Recon Loss: {avg_recon_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "        # Early stopping\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            trigger_times = 0\n",
        "        else:\n",
        "            trigger_times += 1\n",
        "        if trigger_times >= patience:\n",
        "            print(f\"Early stopping at Epoch {epoch+1} for latent_dim={latent_dim}, n_clusters={n_clusters}\")\n",
        "            break\n",
        "    vae.eval()\n",
        "    with torch.no_grad():\n",
        "        _, mu, log_var = vae(torch.FloatTensor(train_data).to(device))\n",
        "        z_train = vae.reparameterize(mu, log_var)\n",
        "    z_train = z_train.cpu().numpy()\n",
        "    clustering = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    train_clusters = clustering.fit_predict(z_train)\n",
        "    if len(np.unique(train_clusters)) > 1:\n",
        "        silhouette = silhouette_score(z_train, train_clusters)\n",
        "        results.append({'latent_dim': latent_dim, 'n_clusters': n_clusters, 'silhouette': silhouette})\n",
        "        print(f\"latent_dim={latent_dim}, n_clusters={n_clusters}, Silhouette Score: {silhouette:.4f}\")\n",
        "best_result = max(results, key=lambda x: x['silhouette'])\n",
        "print(f\"Best Configuration: latent_dim={best_result['latent_dim']}, n_clusters={best_result['n_clusters']}, Silhouette Score: {best_result['silhouette']:.4f}\")\n",
        "best_params = {'latent_dim': best_result['latent_dim'], 'n_clusters': best_result['n_clusters']}\n",
        "\n",
        "# Train final VAE with best parameters\n",
        "vae = VAE(input_dim, output_dim).to(device)\n",
        "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
        "criterion = loss_function\n",
        "vae.train()\n",
        "best_val_loss = float('inf')\n",
        "patience = 20\n",
        "trigger_times = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "for epoch in range(200):\n",
        "    vae.train()\n",
        "    total_loss = 0\n",
        "    total_recon_loss = 0\n",
        "    for i in range(0, len(train_data), 128):\n",
        "        batch = torch.FloatTensor(train_data[i:i+128]).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, log_var = vae(batch)\n",
        "        recon_loss = criterion(recon_batch, batch, mu, log_var)\n",
        "        recon_loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += recon_loss.item()\n",
        "        total_recon_loss += recon_loss.item()\n",
        "    # Validation phase\n",
        "    vae.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(val_data), 128):\n",
        "            val_batch = torch.FloatTensor(val_data[i:i+128]).to(device)\n",
        "            recon_val_batch, _, _ = vae(val_batch)\n",
        "            val_loss += criterion(recon_val_batch, val_batch, mu, log_var).item()\n",
        "    avg_val_loss = val_loss / (len(val_data) / 128)\n",
        "    avg_train_loss = total_loss / (len(train_data) / 128)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/200], Train Recon Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "    # Early stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        trigger_times = 0\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "    if trigger_times >= patience:\n",
        "        print(f\"Early stopping at Epoch {epoch+1}\")\n",
        "        break\n",
        "vae.eval()\n",
        "with torch.no_grad():\n",
        "    _, mu, log_var = vae(torch.FloatTensor(train_data).to(device))\n",
        "    z_train = vae.reparameterize(mu, log_var)\n",
        "z_train = z_train.cpu().numpy()\n",
        "\n",
        "# Process test datasets with initial embeddings and heuristics\n",
        "test_data = {}\n",
        "for name, path in test_paths.items():\n",
        "    df_test = pd.read_csv(path)\n",
        "    df_test['PCode'] = df_test['PCode'].astype(str)\n",
        "    embeddings_test, heuristics_test, commit_ids_test = load_or_generate_embeddings(df_test, path, name)\n",
        "    features_test = np.concatenate([embeddings_test, heuristics_test], axis=1)\n",
        "    test_data[name] = {\n",
        "        'features': features_test,\n",
        "        'heuristics': heuristics_test,\n",
        "        'commit_ids': np.array(commit_ids_test),\n",
        "        'codes': df_test['PCode'].values\n",
        "    }\n",
        "\n",
        "# Recompute recon errors for test data\n",
        "def compute_reconstruction_error(model, data, device, batch_size=128):\n",
        "    model.eval()\n",
        "    recon_errors = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(data), batch_size):\n",
        "            batch = torch.FloatTensor(data[i:i+batch_size]).to(device)\n",
        "            recon_batch, _, _ = model(batch)\n",
        "            error = F.mse_loss(recon_batch, batch, reduction='none').mean(dim=1)\n",
        "            recon_errors.append(error.cpu().numpy())\n",
        "    return np.concatenate(recon_errors)\n"
      ],
      "metadata": {
        "id": "hpLqbgqEPsLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply VAE + clustering to test datasets and analyze\n",
        "heuristic_names = [\n",
        "    \"Missing Null Check\", \"Race Condition\", \"Missing Bounds Check\",\n",
        "    \"Unsafe Memory Allocation\", \"Logging Without Halting\", \"None\"\n",
        "]\n",
        "for name in test_data:\n",
        "    features = test_data[name]['features']\n",
        "    heuristics = test_data[name]['heuristics']\n",
        "    commit_ids = test_data[name]['commit_ids']\n",
        "    codes = test_data[name]['codes']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, mu, log_var = vae(torch.FloatTensor(features).to(device))\n",
        "        z_test = vae.reparameterize(mu, log_var)\n",
        "    z_test = z_test.cpu().numpy()\n",
        "\n",
        "    clustering = AgglomerativeClustering(n_clusters=best_params['n_clusters'], linkage='ward')\n",
        "    clusters = clustering.fit_predict(z_test)\n",
        "\n",
        "    recon_error = compute_reconstruction_error(vae, features, device)\n",
        "\n",
        "    # Save to dictionary\n",
        "    test_data[name].update({\n",
        "        'latent_z': z_test,\n",
        "        'clusters': clusters,\n",
        "        'recon_error': recon_error\n",
        "    })\n",
        "\n",
        "    # Cluster evaluation metrics\n",
        "    silhouette = silhouette_score(z_test, clusters)\n",
        "    ch_score = calinski_harabasz_score(z_test, clusters)\n",
        "    db_score = davies_bouldin_score(z_test, clusters)\n",
        "    print(f\"\\n[{name}] Clustering Metrics:\")\n",
        "    print(f\"Silhouette Score: {silhouette:.4f}, CH: {ch_score:.2f}, DB: {db_score:.4f}\")\n",
        "\n",
        "    # Top anomalies by recon error\n",
        "    print(f\"Top 5 Anomalous Functions in {name}:\")\n",
        "    top_idx = np.argsort(-recon_error)[:5]\n",
        "    for idx in top_idx:\n",
        "        print(f\"Commit ID: {commit_ids[idx]}, Error: {recon_error[idx]:.4f}\\n{codes[idx][:200]}...\\n\")\n",
        "\n",
        "    # Save output dataframe\n",
        "    df_out = pd.DataFrame({\n",
        "        'Commit_ID': commit_ids,\n",
        "        'Cluster': clusters,\n",
        "        'Recon_Error': recon_error,\n",
        "        'Heuristic_Label': [heuristic_names[np.argmax(h) if np.max(h) > 0 else -1] for h in heuristics],\n",
        "        'Code': codes\n",
        "    })\n",
        "    df_out.to_csv(f\"{name}_cluster_results.csv\", index=False)\n",
        "\n",
        "    # Cluster summary\n",
        "    print(f\"Heuristic distribution by cluster in {name}:\")\n",
        "    cluster_heuristic_counts = []\n",
        "    for c in range(best_params['n_clusters']):\n",
        "        idx = np.where(clusters == c)[0]\n",
        "        hsum = np.sum(heuristics[idx], axis=0)\n",
        "        hsum[-1] = np.sum(heuristics[idx, -1] == 0)  # Correct 'None' count\n",
        "        cluster_heuristic_counts.append(hsum)\n",
        "        top = np.argmax(hsum[:-1])  # exclude 'None'\n",
        "        print(f\"Cluster {c}: Dominant Heuristic = {heuristic_names[top]}, Count = {hsum[top]}\")\n",
        "\n",
        "    # t-SNE plot with cluster markers, boundaries, and side labels\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    tsne_coords = tsne.fit_transform(z_test)\n",
        "    tsne_df = pd.DataFrame(tsne_coords, columns=['TSNE1', 'TSNE2'])\n",
        "    tsne_df['Cluster'] = clusters\n",
        "    tsne_df['Heuristic'] = [heuristic_names[np.argmax(h) if np.max(h) > 0 else -1] for h in heuristics]\n",
        "    tsne_df['ReconError'] = recon_error\n",
        "    cluster_df = pd.DataFrame(cluster_heuristic_counts, columns=heuristic_names)\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    scatter = sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue='Heuristic', style='Cluster',\n",
        "                              palette={\n",
        "                                  'Missing Null Check': 'green',\n",
        "                                  'Race Condition': 'red',\n",
        "                                  'Missing Bounds Check': 'blue',\n",
        "                                  'Unsafe Memory Allocation': 'purple',\n",
        "                                  'Logging Without Halting': 'orange',\n",
        "                                  'None': 'gray'\n",
        "                              }, marker='o', s=100)\n",
        "    plt.title(f\"t-SNE of {name} - Regex + GraphCodeBERT + VAE + Kmeans (HYDRA)\")\n",
        "    plt.xlabel(\"t-SNE Dimension 1\")\n",
        "    plt.ylabel(\"t-SNE Dimension 2\")\n",
        "    plt.legend(title=' ', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    # Compute and plot convex hulls for cluster boundaries\n",
        "    from scipy.spatial import ConvexHull\n",
        "    for cluster in range(best_params['n_clusters']):\n",
        "        cluster_indices = np.where(clusters == cluster)[0]\n",
        "        if len(cluster_indices) > 1:  # Need at least 2 points for a hull\n",
        "            cluster_points = tsne_coords[cluster_indices]\n",
        "            hull = ConvexHull(cluster_points)\n",
        "            for simplex in hull.simplices:\n",
        "                plt.plot(cluster_points[simplex, 0], cluster_points[simplex, 1], 'k-', alpha=0.5)\n",
        "\n",
        "    # Label clusters outside the boundaries\n",
        "    for cluster in range(best_params['n_clusters']):\n",
        "        cluster_indices = np.where(clusters == cluster)[0]\n",
        "        if len(cluster_indices) > 0:\n",
        "            cluster_points = tsne_coords[cluster_indices]\n",
        "            cluster_center = cluster_points.mean(axis=0)\n",
        "            x_range = tsne_coords[:, 0].max() - tsne_coords[:, 0].min()\n",
        "            y_range = tsne_coords[:, 1].max() - tsne_coords[:, 1].min()\n",
        "            offset = 0.1 * max(x_range, y_range)\n",
        "            outward_x = cluster_center[0] + offset * np.sign(x_range)\n",
        "            outward_y = cluster_center[1] + offset * np.sign(y_range)\n",
        "            plt.text(outward_x, outward_y, f'Cluster {cluster}',\n",
        "                     fontsize=12, ha='center', va='center', bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{name}_tsne_plot.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Count heuristics per cluster\n",
        "    cluster_heuristics = {i: {h: 0 for h in heuristic_names} for i in range(best_params['n_clusters'])}\n",
        "    for idx, cluster in enumerate(clusters):\n",
        "        for h_idx, h_name in enumerate(heuristic_names):\n",
        "            if heuristics[idx, h_idx] == 1:\n",
        "                cluster_heuristics[cluster][h_name] += 1\n",
        "\n",
        "    # Print total number of each heuristic per cluster\n",
        "    print(f\"\\n{name} Dataset - Cluster Heuristic Counts:\")\n",
        "    for cluster in range(best_params['n_clusters']):\n",
        "        print(f\"\\nCluster {cluster}:\")\n",
        "        for h_name, count in cluster_heuristics[cluster].items():\n",
        "            print(f\"{h_name}: {count}\")\n",
        "\n",
        "    # Detect and print false positives for \"None\" heuristic\n",
        "    print(f\"\\n{name} Dataset - False Positives for 'None' Heuristic:\")\n",
        "    false_positives = []\n",
        "    for idx in range(len(heuristics)):\n",
        "        if heuristics[idx, -1] == 1:  # Sample labeled as \"None\"\n",
        "            if np.any(heuristics[idx, :-1] == 1):  # Check if any other heuristic is true\n",
        "                false_positive_heuristics = [h for h_idx, h in enumerate(heuristic_names[:-1]) if heuristics[idx, h_idx] == 1]\n",
        "                false_positives.append((idx, false_positive_heuristics))\n",
        "\n",
        "    if false_positives:\n",
        "        print(f\"Found {len(false_positives)} false positive(s) where 'None' was incorrectly assigned:\")\n",
        "        for idx, conflicting_heuristics in false_positives:\n",
        "            print(f\"\\nCommit ID: {commit_ids[idx]}\")\n",
        "            print(f\"Conflicting Heuristics: {', '.join(conflicting_heuristics)}\")\n",
        "            print(f\"Code Snippet: {codes[idx][:200]}...\\n\")\n",
        "    else:\n",
        "        print(\"No false positives found for 'None' heuristic.\")"
      ],
      "metadata": {
        "id": "Ce6q9pZ1QMKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "# Dataset and metric setup\n",
        "datasets = list(test_data.keys())\n",
        "metrics = ['Silhouette', 'CH', 'DB']\n",
        "x_locs = np.arange(len(metrics))\n",
        "bar_width = 0.2\n",
        "colors = ['tab:blue', 'tab:orange', 'tab:green']\n",
        "\n",
        "# Step 1: Collect raw scores\n",
        "raw_scores = []\n",
        "for name in datasets:\n",
        "    sil = silhouette_score(test_data[name]['latent_z'], test_data[name]['clusters'])\n",
        "    ch = calinski_harabasz_score(test_data[name]['latent_z'], test_data[name]['clusters']) / 1000\n",
        "    db = davies_bouldin_score(test_data[name]['latent_z'], test_data[name]['clusters'])\n",
        "    raw_scores.append([sil, ch, db])\n",
        "\n",
        "raw_scores = np.array(raw_scores)  # shape: (num_datasets, num_metrics)\n",
        "\n",
        "# Step 2: Normalize for visualization\n",
        "scaler = MinMaxScaler()\n",
        "normalized_scores = scaler.fit_transform(raw_scores)\n",
        "\n",
        "# Add offset to avoid invisible bars\n",
        "normalized_scores += 0.05\n",
        "normalized_scores = np.clip(normalized_scores, 0, 1)\n",
        "\n",
        "# Step 3: Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "for i, (name, color) in enumerate(zip(datasets, colors)):\n",
        "    offset = (i - 1) * bar_width\n",
        "    bars = ax.bar(x_locs + offset, normalized_scores[i], width=bar_width, label=name, color=color)\n",
        "\n",
        "    # Add actual score labels above each bar\n",
        "    for j, bar in enumerate(bars):\n",
        "        height = bar.get_height()\n",
        "        actual_score = raw_scores[i][j]\n",
        "        ax.text(bar.get_x() + bar.get_width() / 2, height + 0.02,\n",
        "                f\"{actual_score:.2f}\", ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Step 4: Axes formatting\n",
        "ax.set_ylabel(\"Normalized Metric Score (0–1)\\n Actual Metric Values Printed Above Each Bar\", fontsize=12)\n",
        "ax.set_xticks(x_locs)\n",
        "ax.set_xticklabels(metrics, fontsize=12)\n",
        "ax.set_title(\"Normalized Clustering Metrics Across Test Datasets (Android, Chrome, IMageMagick) for HYDRA\", fontsize=14)\n",
        "ax.grid(True, axis='y', linestyle='--', alpha=0.4)\n",
        "ax.legend(title=\"Datasets\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"clustering_metrics_improved.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqvBX8P_QdYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "# Ensure train/val losses are NumPy arrays\n",
        "train_losses = np.array(train_losses)\n",
        "val_losses = np.array(val_losses)\n",
        "\n",
        "# Smooth only if sufficient points exist\n",
        "sigma = 1 if len(train_losses) >= 10 else 0\n",
        "train_losses_smooth = gaussian_filter1d(train_losses, sigma=sigma)\n",
        "val_losses_smooth = gaussian_filter1d(val_losses, sigma=sigma)\n",
        "\n",
        "# Best epoch (min validation loss)\n",
        "best_epoch = np.argmin(val_losses)\n",
        "best_loss = val_losses[best_epoch]\n",
        "\n",
        "# Plot style\n",
        "sns.set(style=\"whitegrid\", context='talk')\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(train_losses_smooth, label='Train Loss', color='steelblue', linewidth=2.5)\n",
        "plt.plot(val_losses_smooth, label='Validation Loss', color='darkorange', linewidth=2.5)\n",
        "\n",
        "# Best epoch marker\n",
        "plt.scatter(best_epoch, best_loss, color='red', s=100, zorder=5, label=f'Best Epoch ({best_epoch})')\n",
        "\n",
        "# Annotation near best point\n",
        "y_offset = (val_losses.max() - val_losses.min()) * 0.05\n",
        "plt.annotate(f'{best_loss:.2f}',\n",
        "             xy=(best_epoch, best_loss),\n",
        "             xytext=(best_epoch + 3, best_loss + y_offset),\n",
        "             arrowprops=dict(arrowstyle='->', lw=1.5, color='gray'),\n",
        "             fontsize=12, bbox=dict(boxstyle='round,pad=0.3', fc='white', ec='gray', alpha=0.8))\n",
        "\n",
        "# Labels and title\n",
        "plt.title('VAE Loss Curve During Training (HYDRA)', fontsize=18)\n",
        "plt.xlabel('Epoch', fontsize=14)\n",
        "plt.ylabel('Loss', fontsize=14)\n",
        "\n",
        "plt.legend(fontsize=12, loc='upper right')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.savefig('vae_loss_curve_a_star.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iWQwuSh-QfuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**M3** Model"
      ],
      "metadata": {
        "id": "94kUhk43Qw95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from itertools import product\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import os\n",
        "from collections import Counter\n",
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "# Set reproducibility seed\n",
        "np.random.seed(42)  # NumPy seed\n",
        "torch.manual_seed(42)  # PyTorch seed\n",
        "\n",
        "# GraphCodeBERT Feature Extraction\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
        "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\")\n",
        "\n",
        "def get_code_features(code):\n",
        "    inputs = tokenizer(code, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "# Heuristic Detection Functions\n",
        "def detect_missing_malloc_check(code: str) -> int:\n",
        "    alloc_pattern = r'\\b(\\w+)\\s*\\*\\s*(\\w+)\\s*=\\s*\\([^\\)]*\\)\\s*(malloc|calloc|realloc)\\s*\\([^;]+?\\);|\\b(\\w+)\\s*\\*\\s*(\\w+)\\s*=\\s*(malloc|calloc|realloc)\\s*\\([^;]+?\\);'\n",
        "    matches = re.findall(alloc_pattern, code)\n",
        "    ptrs = [match[1] if match[1] else match[4] for match in matches if match[1] or match[4]]\n",
        "    for ptr in ptrs:\n",
        "        is_checked = False\n",
        "        is_used = False\n",
        "        null_check_patterns = [rf'if\\s*\\(\\s*{ptr}\\s*\\)', rf'if\\s*\\(\\s*{ptr}\\s*!=\\s*NULL\\s*\\)', rf'if\\s*\\(\\s*!{ptr}\\s*\\)', rf'if\\s*\\(\\s*NULL\\s*!=\\s*{ptr}\\s*\\)']\n",
        "        usage_patterns = [rf'{ptr}\\s*\\[', rf'\\*\\s*{ptr}', rf'{ptr}\\s*->', rf'{ptr}\\s*\\(', rf'{ptr}\\s*=']\n",
        "        for pat in null_check_patterns:\n",
        "            if re.search(pat, code):\n",
        "                is_checked = True\n",
        "                break\n",
        "        for pat in usage_patterns:\n",
        "            if re.search(pat, code):\n",
        "                is_used = True\n",
        "                break\n",
        "        if is_used and not is_checked:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def missing_null_check_func(code):\n",
        "    functions = re.findall(r'\\b\\w+\\s+\\w+\\s*\\([^)]*\\)\\s*\\{[^{}]*\\}', code, re.DOTALL)\n",
        "    for func in functions:\n",
        "        ptr_decl = re.findall(r'\\b\\w+\\s*\\*\\s*(\\w+)\\s*=\\s*[^;]+;', func)\n",
        "        ptr_func_decl = re.findall(r'\\b\\w+\\s*\\*\\s*(\\w+)\\s*;', func)\n",
        "        ptr_vars = list(set(ptr_decl + ptr_func_decl))\n",
        "        for ptr in ptr_vars:\n",
        "            unsafe_use = re.search(rf'[^a-zA-Z0-9_]({ptr})(->|\\[\\d*]|[^\\w]?\\*)|\\b{ptr}\\s*\\(', func)\n",
        "            if not unsafe_use:\n",
        "                continue\n",
        "            null_check = re.search(rf'if\\s*\\(\\s*(!\\s*{ptr}|{ptr}\\s*==\\s*NULL|{ptr}\\s*!=\\s*NULL)\\s*\\)', func)\n",
        "            if not null_check:\n",
        "                return 1\n",
        "    return 0\n",
        "\n",
        "def detect_race_condition(code: str) -> bool:\n",
        "    field_assignment_pattern = r'\\b\\w+\\s*(->|\\.)\\s*\\w+\\s*=.*?;'\n",
        "    control_block_pattern = r'\\b(if|while|for|switch)\\s*\\([^\\)]+\\)\\s*{[^}]*' + field_assignment_pattern + r'[^}]*}'\n",
        "    matches_control_blocks = re.findall(control_block_pattern, code, re.DOTALL)\n",
        "    locking_primitive_pattern = r'\\b(mutex_lock|pthread_mutex_lock|spin_lock)\\b'\n",
        "    unlocking_primitive_pattern = r'\\b(mutex_unlock|pthread_mutex_unlock|spin_unlock)\\b'\n",
        "    has_locking = re.search(locking_primitive_pattern, code)\n",
        "    has_unlocking = re.search(unlocking_primitive_pattern, code)\n",
        "    if matches_control_blocks and not (has_locking and has_unlocking):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def logging_but_no_blocking(code):\n",
        "    log_lines = re.findall(r'(syslog\\([^)]*\\)|printk\\([^)]*\\))', code)\n",
        "    for line in log_lines:\n",
        "        log_index = code.find(line)\n",
        "        snippet = code[log_index:log_index+150]\n",
        "        if not re.search(r'(return|exit|break|continue)', snippet):\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def split_into_functions(code):\n",
        "    functions = re.findall(r'([a-zA-Z_][a-zA-Z0-9_\\*\\s]*\\s+[a-zA-Z_][a-zA-Z0-9_]*\\s*\\(.*?\\)\\s*\\{.*?\\})', code, re.DOTALL)\n",
        "    return functions\n",
        "\n",
        "def missing_bounds_check(code):\n",
        "    functions = split_into_functions(code)\n",
        "    risky_keywords = ['recv', 'read', 'strcpy', 'memcpy', 'gets', 'strcat', 'write']\n",
        "    for func in functions:\n",
        "        found_risky = any(kw in func for kw in risky_keywords)\n",
        "        has_check = any('if' in line and ('<' in line or '>' in line or '<=' in line or '>=' in line) for line in func.splitlines())\n",
        "        if found_risky and not has_check:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def analyze_risks(code):\n",
        "    risk_flags = {\n",
        "        \"Missing Null Check\": missing_null_check_func(code),\n",
        "        \"Race Condition\": detect_race_condition(code),\n",
        "        \"Missing Bounds Check\": missing_bounds_check(code),\n",
        "        \"Unsafe Memory Allocation\": detect_missing_malloc_check(code),\n",
        "        \"Logging Without Halting\": logging_but_no_blocking(code),\n",
        "        \"issue_detected\": 0\n",
        "    }\n",
        "    if any(v != 0 for v in risk_flags.values() if v is not False):\n",
        "        risk_flags[\"issue_detected\"] = 1\n",
        "    return risk_flags\n",
        "\n",
        "# Load and preprocess data\n",
        "train_path = '/home/phoenix/Desktop/TA/Re_ New Code/Linux-Data-Updated.csv'\n",
        "test_paths = {\n",
        "    'Android': '/home/phoenix/Desktop/TA/Re_ New Code/CSV Data of three testing projects/Android-Data.csv',\n",
        "    'Chrome': '/home/phoenix/Desktop/TA/Re_ New Code/CSV Data of three testing projects/Chrome-Data.csv',\n",
        "    'ImageMagick': '/home/phoenix/Desktop/TA/Re_ New Code/CSV Data of three testing projects/ImageMagick-Data.csv'\n",
        "}\n",
        "\n",
        "def load_or_generate_embeddings(df, path, filename):\n",
        "    embed_file = f\"{filename}_embeddings.npy\"\n",
        "    heur_file = f\"{filename}_heuristics.npy\"\n",
        "    commit_file = f\"{filename}_commit_ids.npy\"\n",
        "\n",
        "    if os.path.exists(embed_file) and os.path.exists(heur_file) and os.path.exists(commit_file):\n",
        "        embeddings = np.load(embed_file)\n",
        "        heuristics = np.load(heur_file)\n",
        "        commit_ids = np.load(commit_file)\n",
        "    else:\n",
        "        embeddings = []\n",
        "        heuristics = []\n",
        "        commit_ids = []\n",
        "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing GraphCodeBERT embeddings and heuristics ({filename})\"):\n",
        "            code = row['PCode']\n",
        "            embedding = get_code_features(code)\n",
        "            risk_flags = analyze_risks(code)\n",
        "            embeddings.append(embedding)\n",
        "            heuristics.append([risk_flags[h] for h in [\n",
        "                \"Missing Null Check\", \"Race Condition\", \"Missing Bounds Check\",\n",
        "                \"Unsafe Memory Allocation\", \"Logging Without Halting\", \"issue_detected\"\n",
        "            ]])\n",
        "            commit_ids.append(row['Commit_Id'])\n",
        "        embeddings = np.array(embeddings)\n",
        "        heuristics = np.array(heuristics)\n",
        "        np.save(embed_file, embeddings)\n",
        "        np.save(heur_file, heuristics)\n",
        "        np.save(commit_file, commit_ids)\n",
        "    return embeddings, heuristics, commit_ids\n",
        "\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_train['PCode'] = df_train['PCode'].astype(str)\n",
        "embeddings_train, heuristics_train, commit_ids_train = load_or_generate_embeddings(df_train, train_path, \"train\")\n",
        "\n",
        "# Combine embeddings and heuristics for training and split for validation\n",
        "features_train = np.concatenate([embeddings_train, heuristics_train], axis=1)\n",
        "train_data, val_data = train_test_split(features_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Hyperparameter tuning for K-means\n",
        "param_grid = {\n",
        "    'n_clusters': range(2, 5),  # Test 2 to 10 clusters\n",
        "    'n_init': [10, 20, 50],     # Number of initializations\n",
        "    'tol': [1e-4, 1e-5]         # Convergence tolerance\n",
        "}\n",
        "results = []\n",
        "for params in tqdm(product(*param_grid.values()), desc=\"Tuning K-means\"):\n",
        "    n_clusters, n_init, tol = params\n",
        "    clustering = KMeans(n_clusters=n_clusters, init='k-means++', n_init=n_init, tol=tol, random_state=42)\n",
        "    train_clusters = clustering.fit_predict(features_train)\n",
        "    if len(np.unique(train_clusters)) > 1:  # Ensure at least 2 clusters\n",
        "        silhouette = silhouette_score(features_train, train_clusters)\n",
        "        ch_score = calinski_harabasz_score(features_train, train_clusters)\n",
        "        db_score = davies_bouldin_score(features_train, train_clusters)\n",
        "        results.append({\n",
        "            'n_clusters': n_clusters,\n",
        "            'n_init': n_init,\n",
        "            'tol': tol,\n",
        "            'silhouette': silhouette,\n",
        "            'ch_score': ch_score,\n",
        "            'db_score': db_score\n",
        "        })\n",
        "        print(f\"n_clusters={n_clusters}, n_init={n_init}, tol={tol}, Silhouette: {silhouette:.4f}, CH: {ch_score:.2f}, DB: {db_score:.4f}\")\n",
        "\n",
        "# Select best configuration based on silhouette score\n",
        "best_result = max(results, key=lambda x: x['silhouette'])\n",
        "print(f\"Best Configuration: n_clusters={best_result['n_clusters']}, n_init={best_result['n_init']}, tol={best_result['tol']}, \"\n",
        "      f\"Silhouette: {best_result['silhouette']:.4f}, CH: {best_result['ch_score']:.2f}, DB: {best_result['db_score']:.4f}\")\n",
        "best_params = {\n",
        "    'n_clusters': best_result['n_clusters'],\n",
        "    'n_init': best_result['n_init'],\n",
        "    'tol': best_result['tol']\n",
        "}\n",
        "\n",
        "# Process test datasets with initial embeddings and heuristics\n",
        "test_data = {}\n",
        "for name, path in test_paths.items():\n",
        "    df_test = pd.read_csv(path)\n",
        "    df_test['PCode'] = df_test['PCode'].astype(str)\n",
        "    embeddings_test, heuristics_test, commit_ids_test = load_or_generate_embeddings(df_test, path, name)\n",
        "    features_test = np.concatenate([embeddings_test, heuristics_test], axis=1)\n",
        "    test_data[name] = {\n",
        "        'features': features_test,\n",
        "        'heuristics': heuristics_test,\n",
        "        'commit_ids': np.array(commit_ids_test),\n",
        "        'codes': df_test['PCode'].values\n",
        "    }\n",
        "\n",
        "# Apply tuned K-means clustering to test datasets and analyze\n",
        "heuristic_names = [\n",
        "    \"Missing Null Check\", \"Race Condition\", \"Missing Bounds Check\",\n",
        "    \"Unsafe Memory Allocation\", \"Logging Without Halting\", \"None\"\n",
        "]\n",
        "for name in test_data:\n",
        "    features = test_data[name]['features']\n",
        "    heuristics = test_data[name]['heuristics']\n",
        "    commit_ids = test_data[name]['commit_ids']\n",
        "    codes = test_data[name]['codes']\n",
        "\n",
        "    clustering = KMeans(n_clusters=best_params['n_clusters'], init='k-means++', n_init=best_params['n_init'],\n",
        "                        tol=best_params['tol'], random_state=42)\n",
        "    clusters = clustering.fit_predict(features)\n",
        "\n",
        "    # Save to dictionary\n",
        "    test_data[name].update({\n",
        "        'clusters': clusters\n",
        "    })\n",
        "\n",
        "    # Cluster evaluation metrics\n",
        "    silhouette = silhouette_score(features, clusters)\n",
        "    ch_score = calinski_harabasz_score(features, clusters)\n",
        "    db_score = davies_bouldin_score(features, clusters)\n",
        "    print(f\"\\n[{name}] Clustering Metrics:\")\n",
        "    print(f\"Silhouette Score: {silhouette:.4f}, CH: {ch_score:.2f}, DB: {db_score:.4f}\")\n",
        "\n",
        "    # Top anomalies by cluster size\n",
        "    print(f\"Top 5 Largest Clusters in {name}:\")\n",
        "    unique, counts = np.unique(clusters, return_counts=True)\n",
        "    top_clusters = np.argsort(-counts)[:5]\n",
        "    for cluster_idx in top_clusters:\n",
        "        cluster_size = counts[cluster_idx]\n",
        "        cluster_indices = np.where(clusters == unique[cluster_idx])[0]\n",
        "        sample_commit = commit_ids[cluster_indices[0]] if len(cluster_indices) > 0 else \"N/A\"\n",
        "        print(f\"Cluster {unique[cluster_idx]}: Size = {cluster_size}, Sample Commit ID = {sample_commit}\")\n",
        "\n",
        "    # Save output dataframe\n",
        "    df_out = pd.DataFrame({\n",
        "        'Commit_ID': commit_ids,\n",
        "        'Cluster': clusters,\n",
        "        'Heuristic_Label': [heuristic_names[np.argmax(h) if np.max(h) > 0 else -1] for h in heuristics],\n",
        "        'Code': codes\n",
        "    })\n",
        "    df_out.to_csv(f\"{name}_cluster_results.csv\", index=False)\n",
        "\n",
        "    # Cluster summary with full heuristic counts\n",
        "    print(f\"\\nHeuristic distribution by cluster in {name}:\")\n",
        "    cluster_heuristic_counts = []\n",
        "    for c in range(best_params['n_clusters']):\n",
        "        idx = np.where(clusters == c)[0]\n",
        "        hsum = np.sum(heuristics[idx], axis=0)\n",
        "        hsum[-1] = np.sum(heuristics[idx, -1] == 0)  # Correct 'None' count\n",
        "        cluster_heuristic_counts.append(hsum)\n",
        "        top = np.argmax(hsum[:-1])  # Index of dominant heuristic (excluding 'None')\n",
        "        dominant_heuristic = heuristic_names[top]\n",
        "        dominant_count = hsum[top]\n",
        "        print(f\"\\nCluster {c}:\")\n",
        "        print(f\"Dominant Heuristic = {dominant_heuristic}, Count = {dominant_count}\")\n",
        "        for h_idx, h_name in enumerate(heuristic_names):\n",
        "            print(f\"{h_name}: {hsum[h_idx]}\")\n",
        "\n",
        "    # Detect and print false positives for \"None\" heuristic\n",
        "    print(f\"\\n{name} Dataset - False Positives for 'None' Heuristic:\")\n",
        "    false_positives = []\n",
        "    for idx in range(len(heuristics)):\n",
        "        if heuristics[idx, -1] == 1:  # Sample labeled as \"None\"\n",
        "            if np.any(heuristics[idx, :-1] == 1):  # Check if any other heuristic is true\n",
        "                false_positive_heuristics = [h for h_idx, h in enumerate(heuristic_names[:-1]) if heuristics[idx, h_idx] == 1]\n",
        "                false_positives.append((idx, false_positive_heuristics))\n",
        "\n",
        "    if false_positives:\n",
        "        print(f\"Found {len(false_positives)} false positive(s) where 'None' was incorrectly assigned:\")\n",
        "        for idx, conflicting_heuristics in false_positives:\n",
        "            print(f\"\\nCommit ID: {commit_ids[idx]}\")\n",
        "            print(f\"Conflicting Heuristics: {', '.join(conflicting_heuristics)}\")\n",
        "            print(f\"Code Snippet: {codes[idx][:200]}...\\n\")\n",
        "    else:\n",
        "        print(\"No false positives found for 'None' heuristic.\")\n",
        "\n",
        "    # t-SNE plot with cluster markers, boundaries, and side labels\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    tsne_coords = tsne.fit_transform(features)\n",
        "    tsne_df = pd.DataFrame(tsne_coords, columns=['TSNE1', 'TSNE2'])\n",
        "    tsne_df['Cluster'] = clusters\n",
        "    tsne_df['Heuristic'] = [heuristic_names[np.argmax(h) if np.max(h) > 0 else -1] for h in heuristics]\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    scatter = sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue='Heuristic', style='Cluster',\n",
        "                              palette={\n",
        "                                  'Missing Null Check': 'green',\n",
        "                                  'Race Condition': 'red',\n",
        "                                  'Missing Bounds Check': 'blue',\n",
        "                                  'Unsafe Memory Allocation': 'purple',\n",
        "                                  'Logging Without Halting': 'orange',\n",
        "                                  'None': 'gray'\n",
        "                              }, marker='o', s=100)\n",
        "    plt.title(f\"t-SNE of {name} - GraphCodeBERT + Heuristics + Tuned K-means\")\n",
        "    plt.xlabel(\"t-SNE Dimension 1\")\n",
        "    plt.ylabel(\"t-SNE Dimension 2\")\n",
        "    plt.legend(title=' ', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    # Compute and plot convex hulls for cluster boundaries\n",
        "    for cluster in range(best_params['n_clusters']):\n",
        "        cluster_indices = np.where(clusters == cluster)[0]\n",
        "        if len(cluster_indices) > 1:\n",
        "            cluster_points = tsne_coords[cluster_indices]\n",
        "            hull = ConvexHull(cluster_points)\n",
        "            for simplex in hull.simplices:\n",
        "                plt.plot(cluster_points[simplex, 0], cluster_points[simplex, 1], 'k-', alpha=0.5)\n",
        "\n",
        "    # Label clusters outside the boundaries\n",
        "    for cluster in range(best_params['n_clusters']):\n",
        "        cluster_indices = np.where(clusters == cluster)[0]\n",
        "        if len(cluster_indices) > 0:\n",
        "            cluster_points = tsne_coords[cluster_indices]\n",
        "            cluster_center = cluster_points.mean(axis=0)\n",
        "            x_range = tsne_coords[:, 0].max() - tsne_coords[:, 0].min()\n",
        "            y_range = tsne_coords[:, 1].max() - tsne_coords[:, 1].min()\n",
        "            offset = 0.1 * max(x_range, y_range)\n",
        "            outward_x = cluster_center[0] + offset * np.sign(x_range)\n",
        "            outward_y = cluster_center[1] + offset * np.sign(y_range)\n",
        "            plt.text(outward_x, outward_y, f'Cluster {cluster}',\n",
        "                     fontsize=12, ha='center', va='center', bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n"
      ],
      "metadata": {
        "id": "ht7lcDwMQ2FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset and metric setup\n",
        "datasets = list(test_data.keys())\n",
        "metrics = ['Silhouette', 'CH', 'DB']\n",
        "x_locs = np.arange(len(metrics))\n",
        "bar_width = 0.2\n",
        "colors = ['tab:blue', 'tab:orange', 'tab:green']\n",
        "\n",
        "# Step 1: Collect raw scores\n",
        "raw_scores = []\n",
        "for name in datasets:\n",
        "    sil = silhouette_score(test_data[name]['features'], test_data[name]['clusters'])\n",
        "    ch = calinski_harabasz_score(test_data[name]['features'], test_data[name]['clusters']) / 1000\n",
        "    db = davies_bouldin_score(test_data[name]['features'], test_data[name]['clusters'])\n",
        "    raw_scores.append([sil, ch, db])\n",
        "\n",
        "raw_scores = np.array(raw_scores)  # shape: (num_datasets, num_metrics)\n",
        "\n",
        "# Step 2: Normalize for visualization\n",
        "scaler = MinMaxScaler()\n",
        "normalized_scores = scaler.fit_transform(raw_scores)\n",
        "\n",
        "# Add offset to avoid invisible bars\n",
        "normalized_scores += 0.05\n",
        "normalized_scores = np.clip(normalized_scores, 0, 1)\n",
        "\n",
        "# Step 3: Plotting\n",
        "fig, ax = plt.subplots(figsize=(12, 6))  # Increased width to accommodate legend\n",
        "\n",
        "for i, (name, color) in enumerate(zip(datasets, colors)):\n",
        "    offset = (i - 1) * bar_width\n",
        "    bars = ax.bar(x_locs + offset, normalized_scores[i], width=bar_width, label=name, color=color)\n",
        "\n",
        "    # Add actual score labels above each bar\n",
        "    for j, bar in enumerate(bars):\n",
        "        height = bar.get_height()\n",
        "        actual_score = raw_scores[i][j]\n",
        "        ax.text(bar.get_x() + bar.get_width() / 2, height + 0.02,\n",
        "                f\"{actual_score:.2f}\", ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Step 4: Axes formatting\n",
        "ax.set_ylabel(\"Normalized Metric Score (0–1)\\n Actual Metric Values Printed Above Each Bar\", fontsize=12)\n",
        "ax.set_xticks(x_locs)\n",
        "ax.set_xticklabels(metrics, fontsize=12)\n",
        "ax.set_title(\"Normalized Clustering Metrics Across Test Datasets (Android, Chrome, ImageMagick) for GraphCodeBERT + Heuristics + Tuned K-means\", fontsize=14)\n",
        "ax.grid(True, axis='y', linestyle='--', alpha=0.4)\n",
        "ax.legend(title=\"Datasets\", fontsize=10, bbox_to_anchor=(1.15, 1), loc='upper left')  # Adjusted bbox_to_anchor\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"clustering_metrics_improved.png\", dpi=300, bbox_inches='tight')  # Added bbox_inches to ensure legend fits\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pnwVvVUvRN-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**M2** Model"
      ],
      "metadata": {
        "id": "mZ37mfL3RbKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import os\n",
        "from collections import Counter\n",
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "# Set reproducibility seed\n",
        "np.random.seed(42)  # NumPy seed\n",
        "torch.manual_seed(42)  # PyTorch seed\n",
        "\n",
        "# GraphCodeBERT Feature Extraction\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
        "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\")\n",
        "\n",
        "def get_code_features(code):\n",
        "    inputs = tokenizer(code, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "# Load and preprocess data\n",
        "train_path = '/home/phoenix/Desktop/TA/Re_ New Code/Linux-Data-Updated.csv'\n",
        "test_paths = {\n",
        "    'Android': '/home/phoenix/Desktop/TA/Re_ New Code/CSV Data of three testing projects/Android-Data.csv',\n",
        "    'Chrome': '/home/phoenix/Desktop/TA/Re_ New Code/CSV Data of three testing projects/Chrome-Data.csv',\n",
        "    'ImageMagick': '/home/phoenix/Desktop/TA/Re_ New Code/CSV Data of three testing projects/ImageMagick-Data.csv'\n",
        "}\n",
        "\n",
        "def load_or_generate_embeddings(df, path, filename):\n",
        "    embed_file = f\"{filename}_embeddings.npy\"\n",
        "    commit_file = f\"{filename}_commit_ids.npy\"\n",
        "\n",
        "    if os.path.exists(embed_file) and os.path.exists(commit_file):\n",
        "        embeddings = np.load(embed_file)\n",
        "        commit_ids = np.load(commit_file)\n",
        "    else:\n",
        "        embeddings = []\n",
        "        commit_ids = []\n",
        "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing GraphCodeBERT embeddings ({filename})\"):\n",
        "            code = row['PCode']\n",
        "            embedding = get_code_features(code)\n",
        "            embeddings.append(embedding)\n",
        "            commit_ids.append(row['Commit_Id'])\n",
        "        embeddings = np.array(embeddings)\n",
        "        np.save(embed_file, embeddings)\n",
        "        np.save(commit_file, commit_ids)\n",
        "    return embeddings, commit_ids\n",
        "\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_train['PCode'] = df_train['PCode'].astype(str)\n",
        "embeddings_train, commit_ids_train = load_or_generate_embeddings(df_train, train_path, \"train\")\n",
        "\n",
        "# Use embeddings for training and split for validation\n",
        "train_data, val_data = train_test_split(embeddings_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Hyperparameter tuning for K-means\n",
        "param_grid = {\n",
        "    'n_clusters': range(2, 11),  # Test 2 to 10 clusters\n",
        "    'n_init': [10, 20, 50],     # Number of initializations\n",
        "    'tol': [1e-4, 1e-5]         # Convergence tolerance\n",
        "}\n",
        "results = []\n",
        "for params in tqdm(product(*param_grid.values()), desc=\"Tuning K-means\"):\n",
        "    n_clusters, n_init, tol = params\n",
        "    clustering = KMeans(n_clusters=n_clusters, init='k-means++', n_init=n_init, tol=tol, random_state=42)\n",
        "    train_clusters = clustering.fit_predict(train_data)\n",
        "    if len(np.unique(train_clusters)) > 1:\n",
        "        silhouette = silhouette_score(train_data, train_clusters)\n",
        "        ch_score = calinski_harabasz_score(train_data, train_clusters)\n",
        "        db_score = davies_bouldin_score(train_data, train_clusters)\n",
        "        results.append({\n",
        "            'n_clusters': n_clusters,\n",
        "            'n_init': n_init,\n",
        "            'tol': tol,\n",
        "            'silhouette': silhouette,\n",
        "            'ch_score': ch_score,\n",
        "            'db_score': db_score\n",
        "        })\n",
        "        print(f\"n_clusters={n_clusters}, n_init={n_init}, tol={tol}, Silhouette: {silhouette:.4f}, CH: {ch_score:.2f}, DB: {db_score:.4f}\")\n",
        "\n",
        "# Select best configuration based on silhouette score\n",
        "best_result = max(results, key=lambda x: x['silhouette'])\n",
        "print(f\"Best Configuration: n_clusters={best_result['n_clusters']}, n_init={best_result['n_init']}, tol={best_result['tol']}, \"\n",
        "      f\"Silhouette: {best_result['silhouette']:.4f}, CH: {best_result['ch_score']:.2f}, DB: {best_result['db_score']:.4f}\")\n",
        "best_params = {\n",
        "    'n_clusters': best_result['n_clusters'],\n",
        "    'n_init': best_result['n_init'],\n",
        "    'tol': best_result['tol']\n",
        "}\n",
        "\n",
        "# Process test datasets with embeddings\n",
        "test_data = {}\n",
        "for name, path in test_paths.items():\n",
        "    df_test = pd.read_csv(path)\n",
        "    df_test['PCode'] = df_test['PCode'].astype(str)\n",
        "    embeddings_test, commit_ids_test = load_or_generate_embeddings(df_test, path, name)\n",
        "    test_data[name] = {\n",
        "        'features': embeddings_test,\n",
        "        'commit_ids': np.array(commit_ids_test),\n",
        "        'codes': df_test['PCode'].values\n",
        "    }\n",
        "\n",
        "# Apply tuned K-means clustering to test datasets and analyze\n",
        "for name in test_data:\n",
        "    features = test_data[name]['features']\n",
        "    commit_ids = test_data[name]['commit_ids']\n",
        "    codes = test_data[name]['codes']\n",
        "\n",
        "    clustering = KMeans(n_clusters=best_params['n_clusters'], init='k-means++', n_init=best_params['n_init'],\n",
        "                        tol=best_params['tol'], random_state=42)\n",
        "    clusters = clustering.fit_predict(features)\n",
        "\n",
        "    # Save to dictionary\n",
        "    test_data[name].update({\n",
        "        'clusters': clusters\n",
        "    })\n",
        "\n",
        "    # Cluster evaluation metrics\n",
        "    silhouette = silhouette_score(features, clusters)\n",
        "    ch_score = calinski_harabasz_score(features, clusters)\n",
        "    db_score = davies_bouldin_score(features, clusters)\n",
        "    print(f\"\\n[{name}] Clustering Metrics:\")\n",
        "    print(f\"Silhouette Score: {silhouette:.4f}, CH: {ch_score:.2f}, DB: {db_score:.4f}\")\n",
        "\n",
        "    # Top anomalies by cluster size\n",
        "    print(f\"Top 5 Largest Clusters in {name}:\")\n",
        "    unique, counts = np.unique(clusters, return_counts=True)\n",
        "    top_clusters = np.argsort(-counts)[:5]\n",
        "    for cluster_idx in top_clusters:\n",
        "        cluster_size = counts[cluster_idx]\n",
        "        cluster_indices = np.where(clusters == unique[cluster_idx])[0]\n",
        "        sample_commit = commit_ids[cluster_indices[0]] if len(cluster_indices) > 0 else \"N/A\"\n",
        "        print(f\"Cluster {unique[cluster_idx]}: Size = {cluster_size}, Sample Commit ID = {sample_commit}\")\n",
        "\n",
        "    # Save output dataframe\n",
        "    df_out = pd.DataFrame({\n",
        "        'Commit_ID': commit_ids,\n",
        "        'Cluster': clusters,\n",
        "        'Code': codes\n",
        "    })\n",
        "    df_out.to_csv(f\"{name}_cluster_results.csv\", index=False)\n",
        "\n",
        "    # t-SNE plot with cluster markers, boundaries, and side labels\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    tsne_coords = tsne.fit_transform(features)\n",
        "    tsne_df = pd.DataFrame(tsne_coords, columns=['TSNE1', 'TSNE2'])\n",
        "    tsne_df['Cluster'] = clusters\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    scatter = sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue='Cluster', style='Cluster',\n",
        "                              palette='tab10', marker='o', s=100)\n",
        "    plt.title(f\"t-SNE of {name} - GraphCodeBERT + Tuned K-means\")\n",
        "    plt.xlabel(\"t-SNE Dimension 1\")\n",
        "    plt.ylabel(\"t-SNE Dimension 2\")\n",
        "    plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    # Compute and plot convex hulls for cluster boundaries with error handling\n",
        "    for cluster in range(best_params['n_clusters']):\n",
        "        cluster_indices = np.where(clusters == cluster)[0]\n",
        "        if len(cluster_indices) > 2:\n",
        "            cluster_points = tsne_coords[cluster_indices]\n",
        "            if np.linalg.norm(cluster_points.max(axis=0) - cluster_points.min(axis=0)) > 1e-10:\n",
        "                try:\n",
        "                    hull = ConvexHull(cluster_points)\n",
        "                    for simplex in hull.simplices:\n",
        "                        plt.plot(cluster_points[simplex, 0], cluster_points[simplex, 1], 'k-', alpha=0.5)\n",
        "                except QhullError as e:\n",
        "                    print(f\"Warning: ConvexHull failed for Cluster {cluster} in {name}: {e}\")\n",
        "            else:\n",
        "                print(f\"Warning: Insufficient variance in Cluster {cluster} in {name}, skipping hull.\")\n",
        "\n",
        "    # Label clusters outside the boundaries\n",
        "    for cluster in range(best_params['n_clusters']):\n",
        "        cluster_indices = np.where(clusters == cluster)[0]\n",
        "        if len(cluster_indices) > 0:\n",
        "            cluster_points = tsne_coords[cluster_indices]\n",
        "            cluster_center = cluster_points.mean(axis=0)\n",
        "            x_range = tsne_coords[:, 0].max() - tsne_coords[:, 0].min()\n",
        "            y_range = tsne_coords[:, 1].max() - tsne_coords[:, 1].min()\n",
        "            offset = 0.1 * max(x_range, y_range)\n",
        "            outward_x = cluster_center[0] + offset * np.sign(x_range)\n",
        "            outward_y = cluster_center[1] + offset * np.sign(y_range)\n",
        "            plt.text(outward_x, outward_y, f'Cluster {cluster}',\n",
        "                     fontsize=12, ha='center', va='center', bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{name}_tsne_plot.png\")\n",
        "    plt.show()\n",
        "\n",
        "# Combined metric plot (corrected to normalized style with legend on right)\n",
        "datasets = list(test_data.keys())\n",
        "metrics = ['Silhouette', 'CH', 'DB']\n",
        "x_locs = np.arange(len(metrics))\n",
        "bar_width = 0.2\n",
        "colors = ['tab:blue', 'tab:orange', 'tab:green']\n",
        "\n",
        "# Collect raw scores\n",
        "raw_scores = []\n",
        "for name in datasets:\n",
        "    sil = silhouette_score(test_data[name]['features'], test_data[name]['clusters'])\n",
        "    ch = calinski_harabasz_score(test_data[name]['features'], test_data[name]['clusters']) / 1000\n",
        "    db = davies_bouldin_score(test_data[name]['features'], test_data[name]['clusters'])\n",
        "    raw_scores.append([sil, ch, db])\n",
        "\n",
        "raw_scores = np.array(raw_scores)\n",
        "\n",
        "# Normalize for visualization\n",
        "scaler = MinMaxScaler()\n",
        "normalized_scores = scaler.fit_transform(raw_scores)\n",
        "\n",
        "# Add offset to avoid invisible bars\n",
        "normalized_scores += 0.05\n",
        "normalized_scores = np.clip(normalized_scores, 0, 1)\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(12, 6))  # Increased width for legend\n",
        "\n",
        "for i, (name, color) in enumerate(zip(datasets, colors)):\n",
        "    offset = (i - 1) * bar_width\n",
        "    bars = ax.bar(x_locs + offset, normalized_scores[i], width=bar_width, label=name, color=color)\n",
        "\n",
        "    # Add actual score labels above each bar\n",
        "    for j, bar in enumerate(bars):\n",
        "        height = bar.get_height()\n",
        "        actual_score = raw_scores[i][j]\n",
        "        ax.text(bar.get_x() + bar.get_width() / 2, height + 0.02,\n",
        "                f\"{actual_score:.2f}\", ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Axes formatting\n",
        "ax.set_ylabel(\"Normalized Metric Score (0–1)\\n Actual Metric Values Printed Above Each Bar\", fontsize=12)\n",
        "ax.set_xticks(x_locs)\n",
        "ax.set_xticklabels(metrics, fontsize=12)\n",
        "ax.set_title(\"Normalized Clustering Metrics Across Test Datasets (Android, Chrome, ImageMagick) for GraphCodeBERT + Tuned K-means\", fontsize=14)\n",
        "ax.grid(True, axis='y', linestyle='--', alpha=0.4)\n",
        "ax.legend(title=\"Datasets\", fontsize=10, bbox_to_anchor=(1.15, 1), loc='upper left')  # Legend on right\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"clustering_metrics_improved.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NuG5hpIYRgoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**M1** Model"
      ],
      "metadata": {
        "id": "glg7FzawSIfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from itertools import product\n",
        "import os\n",
        "from collections import Counter\n",
        "from scipy.spatial import ConvexHull, QhullError  # Added QhullError import\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Set reproducibility seed\n",
        "np.random.seed(42)  # NumPy seed\n",
        "\n",
        "# Heuristic Detection Functions\n",
        "def detect_missing_malloc_check(code: str) -> int:\n",
        "    alloc_pattern = r'\\b(\\w+)\\s*\\*\\s*(\\w+)\\s*=\\s*\\([^\\)]*\\)\\s*(malloc|calloc|realloc)\\s*\\([^;]+?\\);|\\b(\\w+)\\s*\\*\\s*(\\w+)\\s*=\\s*(malloc|calloc|realloc)\\s*\\([^;]+?\\);'\n",
        "    matches = re.findall(alloc_pattern, code)\n",
        "    ptrs = [match[1] if match[1] else match[4] for match in matches if match[1] or match[4]]\n",
        "    for ptr in ptrs:\n",
        "        is_checked = False\n",
        "        is_used = False\n",
        "        null_check_patterns = [rf'if\\s*\\(\\s*{ptr}\\s*\\)', rf'if\\s*\\(\\s*{ptr}\\s*!=\\s*NULL\\s*\\)', rf'if\\s*\\(\\s*!{ptr}\\s*\\)', rf'if\\s*\\(\\s*NULL\\s*!=\\s*{ptr}\\s*\\)']\n",
        "        usage_patterns = [rf'{ptr}\\s*\\[', rf'\\*\\s*{ptr}', rf'{ptr}\\s*->', rf'{ptr}\\s*\\(', rf'{ptr}\\s*=']\n",
        "        for pat in null_check_patterns:\n",
        "            if re.search(pat, code):\n",
        "                is_checked = True\n",
        "                break\n",
        "        for pat in usage_patterns:\n",
        "            if re.search(pat, code):\n",
        "                is_used = True\n",
        "                break\n",
        "        if is_used and not is_checked:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def missing_null_check_func(code):\n",
        "    functions = re.findall(r'\\b\\w+\\s+\\w+\\s*\\([^)]*\\)\\s*\\{[^{}]*\\}', code, re.DOTALL)\n",
        "    for func in functions:\n",
        "        ptr_decl = re.findall(r'\\b\\w+\\s*\\*\\s*(\\w+)\\s*=\\s*[^;]+;', func)\n",
        "        ptr_func_decl = re.findall(r'\\b\\w+\\s*\\*\\s*(\\w+)\\s*;', func)\n",
        "        ptr_vars = list(set(ptr_decl + ptr_func_decl))\n",
        "        for ptr in ptr_vars:\n",
        "            unsafe_use = re.search(rf'[^a-zA-Z0-9_]({ptr})(->|\\[\\d*]|[^\\w]?\\*)|\\b{ptr}\\s*\\(', func)\n",
        "            if not unsafe_use:\n",
        "                continue\n",
        "            null_check = re.search(rf'if\\s*\\(\\s*(!\\s*{ptr}|{ptr}\\s*==\\s*NULL|{ptr}\\s*!=\\s*NULL)\\s*\\)', func)\n",
        "            if not null_check:\n",
        "                return 1\n",
        "    return 0\n",
        "\n",
        "def detect_race_condition(code: str) -> bool:\n",
        "    field_assignment_pattern = r'\\b\\w+\\s*(->|\\.)\\s*\\w+\\s*=.*?;'\n",
        "    control_block_pattern = r'\\b(if|while|for|switch)\\s*\\([^\\)]+\\)\\s*{[^}]*' + field_assignment_pattern + r'[^}]*}'\n",
        "    matches_control_blocks = re.findall(control_block_pattern, code, re.DOTALL)\n",
        "    locking_primitive_pattern = r'\\b(mutex_lock|pthread_mutex_lock|spin_lock)\\b'\n",
        "    unlocking_primitive_pattern = r'\\b(mutex_unlock|pthread_mutex_unlock|spin_unlock)\\b'\n",
        "    has_locking = re.search(locking_primitive_pattern, code)\n",
        "    has_unlocking = re.search(unlocking_primitive_pattern, code)\n",
        "    if matches_control_blocks and not (has_locking and has_unlocking):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def logging_but_no_blocking(code):\n",
        "    log_lines = re.findall(r'(syslog\\([^)]*\\)|printk\\([^)]*\\))', code)\n",
        "    for line in log_lines:\n",
        "        log_index = code.find(line)\n",
        "        snippet = code[log_index:log_index+150]\n",
        "        if not re.search(r'(return|exit|break|continue)', snippet):\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def split_into_functions(code):\n",
        "    functions = re.findall(r'([a-zA-Z_][a-zA-Z0-9_\\*\\s]*\\s+[a-zA-Z_][a-zA-Z0-9_]*\\s*\\(.*?\\)\\s*\\{.*?\\})', code, re.DOTALL)\n",
        "    return functions\n",
        "\n",
        "def missing_bounds_check(code):\n",
        "    functions = split_into_functions(code)\n",
        "    risky_keywords = ['recv', 'read', 'strcpy', 'memcpy', 'gets', 'strcat', 'write']\n",
        "    for func in functions:\n",
        "        found_risky = any(kw in func for kw in risky_keywords)\n",
        "        has_check = any('if' in line and ('<' in line or '>' in line or '<=' in line or '>=' in line) for line in func.splitlines())\n",
        "        if found_risky and not has_check:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def analyze_risks(code):\n",
        "    risk_flags = {\n",
        "        \"Missing Null Check\": missing_null_check_func(code),\n",
        "        \"Race Condition\": detect_race_condition(code),\n",
        "        \"Missing Bounds Check\": missing_bounds_check(code),\n",
        "        \"Unsafe Memory Allocation\": detect_missing_malloc_check(code),\n",
        "        \"Logging Without Halting\": logging_but_no_blocking(code),\n",
        "        \"issue_detected\": 0\n",
        "    }\n",
        "    if any(v != 0 for v in risk_flags.values() if v is not False):\n",
        "        risk_flags[\"issue_detected\"] = 1\n",
        "    return risk_flags\n",
        "\n",
        "# Load and preprocess data\n",
        "train_path = '/home/phoenix/Desktop/TA/Re_ New Code/Linux-Data-Updated.csv'\n",
        "test_paths = {\n",
        "    'Android': '/home/phoenix/Desktop/TA/Re_ New Code/CSV Data of three testing projects/Android-Data.csv',\n",
        "    'Chrome': '/home/phoenix/Desktop/TA/Re_ New Code/CSV Data of three testing projects/Chrome-Data.csv',\n",
        "    'ImageMagick': '/home/phoenix/Desktop/TA/Re_ New Code/CSV Data of three testing projects/ImageMagick-Data.csv'\n",
        "}\n",
        "\n",
        "def load_or_generate_heuristics(df, path, filename):\n",
        "    heur_file = f\"{filename}_heuristics.npy\"\n",
        "    commit_file = f\"{filename}_commit_ids.npy\"\n",
        "\n",
        "    if os.path.exists(heur_file) and os.path.exists(commit_file):\n",
        "        heuristics = np.load(heur_file)\n",
        "        commit_ids = np.load(commit_file)\n",
        "    else:\n",
        "        heuristics = []\n",
        "        commit_ids = []\n",
        "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing heuristics ({filename})\"):\n",
        "            code = row['PCode']\n",
        "            risk_flags = analyze_risks(code)\n",
        "            heuristics.append([risk_flags[h] for h in [\n",
        "                \"Missing Null Check\", \"Race Condition\", \"Missing Bounds Check\",\n",
        "                \"Unsafe Memory Allocation\", \"Logging Without Halting\", \"issue_detected\"\n",
        "            ]])\n",
        "            commit_ids.append(row['Commit_Id'])\n",
        "        heuristics = np.array(heuristics)\n",
        "        np.save(heur_file, heuristics)\n",
        "        np.save(commit_file, commit_ids)\n",
        "    return heuristics, commit_ids\n",
        "\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_train['PCode'] = df_train['PCode'].astype(str)\n",
        "heuristics_train, commit_ids_train = load_or_generate_heuristics(df_train, train_path, \"train\")\n",
        "\n",
        "# Use heuristics for training and split for validation\n",
        "train_data, val_data = train_test_split(heuristics_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Hyperparameter tuning for K-means\n",
        "param_grid = {\n",
        "    'n_clusters': range(2, 7),  # Test 2 to 10 clusters\n",
        "    'n_init': [10, 20, 50],     # Number of initializations\n",
        "    'tol': [1e-4, 1e-5]         # Convergence tolerance\n",
        "}\n",
        "results = []\n",
        "for params in tqdm(product(*param_grid.values()), desc=\"Tuning K-means\"):\n",
        "    n_clusters, n_init, tol = params\n",
        "    clustering = KMeans(n_clusters=n_clusters, init='k-means++', n_init=n_init, tol=tol, random_state=42)\n",
        "    train_clusters = clustering.fit_predict(train_data)\n",
        "    if len(np.unique(train_clusters)) > 1:\n",
        "        silhouette = silhouette_score(train_data, train_clusters)\n",
        "        ch_score = calinski_harabasz_score(train_data, train_clusters)\n",
        "        db_score = davies_bouldin_score(train_data, train_clusters)\n",
        "        results.append({\n",
        "            'n_clusters': n_clusters,\n",
        "            'n_init': n_init,\n",
        "            'tol': tol,\n",
        "            'silhouette': silhouette,\n",
        "            'ch_score': ch_score,\n",
        "            'db_score': db_score\n",
        "        })\n",
        "        print(f\"n_clusters={n_clusters}, n_init={n_init}, tol={tol}, Silhouette: {silhouette:.4f}, CH: {ch_score:.2f}, DB: {db_score:.4f}\")\n",
        "\n",
        "# Select best configuration based on silhouette score\n",
        "best_result = max(results, key=lambda x: x['silhouette'])\n",
        "print(f\"Best Configuration: n_clusters={best_result['n_clusters']}, n_init={best_result['n_init']}, tol={best_result['tol']}, \"\n",
        "      f\"Silhouette: {best_result['silhouette']:.4f}, CH: {best_result['ch_score']:.2f}, DB: {best_result['db_score']:.4f}\")\n",
        "best_params = {\n",
        "    'n_clusters': best_result['n_clusters'],\n",
        "    'n_init': best_result['n_init'],\n",
        "    'tol': best_result['tol']\n",
        "}\n",
        "\n",
        "# Process test datasets with heuristics\n",
        "test_data = {}\n",
        "for name, path in test_paths.items():\n",
        "    df_test = pd.read_csv(path)\n",
        "    df_test['PCode'] = df_test['PCode'].astype(str)\n",
        "    heuristics_test, commit_ids_test = load_or_generate_heuristics(df_test, path, name)\n",
        "    test_data[name] = {\n",
        "        'features': heuristics_test,\n",
        "        'heuristics': heuristics_test,\n",
        "        'commit_ids': np.array(commit_ids_test),\n",
        "        'codes': df_test['PCode'].values\n",
        "    }\n",
        "\n",
        "# Apply tuned K-means clustering to test datasets and analyze\n",
        "heuristic_names = [\n",
        "    \"Missing Null Check\", \"Race Condition\", \"Missing Bounds Check\",\n",
        "    \"Unsafe Memory Allocation\", \"Logging Without Halting\", \"None\"\n",
        "]\n",
        "for name in test_data:\n",
        "    features = test_data[name]['features']\n",
        "    heuristics = test_data[name]['heuristics']\n",
        "    commit_ids = test_data[name]['commit_ids']\n",
        "    codes = test_data[name]['codes']\n",
        "\n",
        "    clustering = KMeans(n_clusters=best_params['n_clusters'], init='k-means++', n_init=best_params['n_init'],\n",
        "                        tol=best_params['tol'], random_state=42)\n",
        "    clusters = clustering.fit_predict(features)\n",
        "\n",
        "    # Save to dictionary\n",
        "    test_data[name].update({\n",
        "        'clusters': clusters\n",
        "    })\n",
        "\n",
        "    # Cluster evaluation metrics\n",
        "    silhouette = silhouette_score(features, clusters)\n",
        "    ch_score = calinski_harabasz_score(features, clusters)\n",
        "    db_score = davies_bouldin_score(features, clusters)\n",
        "    print(f\"\\n[{name}] Clustering Metrics:\")\n",
        "    print(f\"Silhouette Score: {silhouette:.4f}, CH: {ch_score:.2f}, DB: {db_score:.4f}\")\n",
        "\n",
        "    # Top anomalies by cluster size\n",
        "    print(f\"Top 5 Largest Clusters in {name}:\")\n",
        "    unique, counts = np.unique(clusters, return_counts=True)\n",
        "    top_clusters = np.argsort(-counts)[:5]\n",
        "    for cluster_idx in top_clusters:\n",
        "        cluster_size = counts[cluster_idx]\n",
        "        cluster_indices = np.where(clusters == unique[cluster_idx])[0]\n",
        "        sample_commit = commit_ids[cluster_indices[0]] if len(cluster_indices) > 0 else \"N/A\"\n",
        "        print(f\"Cluster {unique[cluster_idx]}: Size = {cluster_size}, Sample Commit ID = {sample_commit}\")\n",
        "\n",
        "    # Save output dataframe\n",
        "    df_out = pd.DataFrame({\n",
        "        'Commit_ID': commit_ids,\n",
        "        'Cluster': clusters,\n",
        "        'Heuristic_Label': [heuristic_names[np.argmax(h) if np.max(h) > 0 else -1] for h in heuristics],\n",
        "        'Code': codes\n",
        "    })\n",
        "    df_out.to_csv(f\"{name}_cluster_results.csv\", index=False)\n",
        "\n",
        "    # Cluster summary with heuristic distribution\n",
        "    print(f\"\\nHeuristic distribution by cluster in {name}:\")\n",
        "    cluster_heuristic_counts = []\n",
        "    for c in range(best_params['n_clusters']):\n",
        "        idx = np.where(clusters == c)[0]\n",
        "        hsum = np.sum(heuristics[idx], axis=0)\n",
        "        hsum[-1] = np.sum(heuristics[idx, -1] == 0)  # Correct 'None' count\n",
        "        cluster_heuristic_counts.append(hsum)\n",
        "        top = np.argmax(hsum[:-1])  # Index of dominant heuristic (excluding 'None')\n",
        "        dominant_heuristic = heuristic_names[top]\n",
        "        dominant_count = hsum[top]\n",
        "        print(f\"\\nCluster {c}:\")\n",
        "        print(f\"Dominant Heuristic = {dominant_heuristic}, Count = {dominant_count}\")\n",
        "        for h_idx, h_name in enumerate(heuristic_names):\n",
        "            print(f\"{h_name}: {hsum[h_idx]}\")\n",
        "\n",
        "    # Detect and print false positives for \"None\" heuristic\n",
        "    print(f\"\\n{name} Dataset - False Positives for 'None' Heuristic:\")\n",
        "    false_positives = []\n",
        "    for idx in range(len(heuristics)):\n",
        "        if heuristics[idx, -1] == 1:  # Sample labeled as \"None\"\n",
        "            if np.any(heuristics[idx, :-1] == 1):  # Check if any other heuristic is true\n",
        "                false_positive_heuristics = [h for h_idx, h in enumerate(heuristic_names[:-1]) if heuristics[idx, h_idx] == 1]\n",
        "                false_positives.append((idx, false_positive_heuristics))\n",
        "\n",
        "    if false_positives:\n",
        "        print(f\"Found {len(false_positives)} false positive(s) where 'None' was incorrectly assigned:\")\n",
        "        for idx, conflicting_heuristics in false_positives:\n",
        "            print(f\"\\nCommit ID: {commit_ids[idx]}\")\n",
        "            print(f\"Conflicting Heuristics: {', '.join(conflicting_heuristics)}\")\n",
        "            print(f\"Code Snippet: {codes[idx][:200]}...\\n\")\n",
        "    else:\n",
        "        print(\"No false positives found for 'None' heuristic.\")\n",
        "\n",
        "    # t-SNE plot with cluster markers, boundaries, and side labels\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    tsne_coords = tsne.fit_transform(features)\n",
        "    tsne_df = pd.DataFrame(tsne_coords, columns=['TSNE1', 'TSNE2'])\n",
        "    tsne_df['Cluster'] = clusters\n",
        "    tsne_df['Heuristic'] = [heuristic_names[np.argmax(h) if np.max(h) > 0 else -1] for h in heuristics]\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    scatter = sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue='Heuristic', style='Cluster',\n",
        "                              palette={\n",
        "                                  'Missing Null Check': 'green',\n",
        "                                  'Race Condition': 'red',\n",
        "                                  'Missing Bounds Check': 'blue',\n",
        "                                  'Unsafe Memory Allocation': 'purple',\n",
        "                                  'Logging Without Halting': 'orange',\n",
        "                                  'None': 'gray'\n",
        "                              }, marker='o', s=100)\n",
        "    plt.title(f\"t-SNE of {name} - Heuristics + Tuned K-means\")\n",
        "    plt.xlabel(\"t-SNE Dimension 1\")\n",
        "    plt.ylabel(\"t-SNE Dimension 2\")\n",
        "    plt.legend(title=' ', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    # Compute and plot convex hulls for cluster boundaries with error handling\n",
        "    for cluster in range(best_params['n_clusters']):\n",
        "        cluster_indices = np.where(clusters == cluster)[0]\n",
        "        if len(cluster_indices) > 2:  # Require at least 3 points for a 2D hull\n",
        "            cluster_points = tsne_coords[cluster_indices]\n",
        "            # Check for sufficient variance to avoid collinearity\n",
        "            if np.linalg.norm(cluster_points.max(axis=0) - cluster_points.min(axis=0)) > 1e-10:\n",
        "                try:\n",
        "                    hull = ConvexHull(cluster_points)\n",
        "                    for simplex in hull.simplices:\n",
        "                        plt.plot(cluster_points[simplex, 0], cluster_points[simplex, 1], 'k-', alpha=0.5)\n",
        "                except QhullError as e:\n",
        "                    print(f\"Warning: ConvexHull failed for Cluster {cluster} in {name} due to coplanarity or low dimensionality: {e}\")\n",
        "            else:\n",
        "                print(f\"Warning: Insufficient variance in Cluster {cluster} in {name}, skipping hull.\")\n",
        "\n",
        "    # Label clusters outside the boundaries\n",
        "    for cluster in range(best_params['n_clusters']):\n",
        "        cluster_indices = np.where(clusters == cluster)[0]\n",
        "        if len(cluster_indices) > 0:\n",
        "            cluster_points = tsne_coords[cluster_indices]\n",
        "            cluster_center = cluster_points.mean(axis=0)\n",
        "            x_range = tsne_coords[:, 0].max() - tsne_coords[:, 0].min()\n",
        "            y_range = tsne_coords[:, 1].max() - tsne_coords[:, 1].min()\n",
        "            offset = 0.1 * max(x_range, y_range)\n",
        "            outward_x = cluster_center[0] + offset * np.sign(x_range)\n",
        "            outward_y = cluster_center[1] + offset * np.sign(y_range)\n",
        "            plt.text(outward_x, outward_y, f'Cluster {cluster}',\n",
        "                     fontsize=12, ha='center', va='center', bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "\n",
        "\n",
        "# Combined metric plot (corrected to normalized style with legend on right)\n",
        "datasets = list(test_data.keys())\n",
        "metrics = ['Silhouette', 'CH', 'DB']\n",
        "x_locs = np.arange(len(metrics))\n",
        "bar_width = 0.2\n",
        "colors = ['tab:blue', 'tab:orange', 'tab:green']\n",
        "\n",
        "# Collect raw scores\n",
        "raw_scores = []\n",
        "for name in datasets:\n",
        "    sil = silhouette_score(test_data[name]['features'], test_data[name]['clusters'])\n",
        "    ch = calinski_harabasz_score(test_data[name]['features'], test_data[name]['clusters']) / 1000\n",
        "    db = davies_bouldin_score(test_data[name]['features'], test_data[name]['clusters'])\n",
        "    raw_scores.append([sil, ch, db])\n",
        "\n",
        "raw_scores = np.array(raw_scores)\n",
        "\n",
        "# Normalize for visualization\n",
        "scaler = MinMaxScaler()\n",
        "normalized_scores = scaler.fit_transform(raw_scores)\n",
        "\n",
        "# Add offset to avoid invisible bars\n",
        "normalized_scores += 0.05\n",
        "normalized_scores = np.clip(normalized_scores, 0, 1)\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(12, 6))  # Increased width for legend\n",
        "\n",
        "for i, (name, color) in enumerate(zip(datasets, colors)):\n",
        "    offset = (i - 1) * bar_width\n",
        "    bars = ax.bar(x_locs + offset, normalized_scores[i], width=bar_width, label=name, color=color)\n",
        "\n",
        "    # Add actual score labels above each bar\n",
        "    for j, bar in enumerate(bars):\n",
        "        height = bar.get_height()\n",
        "        actual_score = raw_scores[i][j]\n",
        "        ax.text(bar.get_x() + bar.get_width() / 2, height + 0.02,\n",
        "                f\"{actual_score:.2f}\", ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Axes formatting\n",
        "ax.set_ylabel(\"Normalized Metric Score (0–1)\\n Actual Metric Values Printed Above Each Bar\", fontsize=12)\n",
        "ax.set_xticks(x_locs)\n",
        "ax.set_xticklabels(metrics, fontsize=12)\n",
        "ax.set_title(\"Normalized Clustering Metrics Across Test Datasets (Android, Chrome, ImageMagick) for Heuristics + Tuned K-means\", fontsize=14)\n",
        "ax.grid(True, axis='y', linestyle='--', alpha=0.4)\n",
        "ax.legend(title=\"Datasets\", fontsize=10, bbox_to_anchor=(1.15, 1), loc='upper left')  # Legend on right\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"clustering_metrics_improved.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zs5mWTlqR18w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}